{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: center; margin: 0;\"> Міністерство освіти і науки України </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> Національний технічний університет України </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> «Київський політехнічний інститут імені Ігоря Сікорського» </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> «Інститут прикладного системного аналізу» </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> Кафедра математичних методів системного аналізу </p>\n",
    "<br>\n",
    "\n",
    "## <p style=\"text-align: center; margin: 0;\"> Звіт </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> про виконання лабораторної роботи №2 </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> з дисципліни </p>\n",
    "### <p style=\"text-align: center; margin: 0;\"> «Розпізнавання образів» </p>\n",
    "<br>\n",
    "\n",
    "### <p style=\"text-align: right; margin: 0;\"> Виконали: студенти ІV курсу &emsp;&emsp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> групи  КА-76  &emsp;&emsp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> Панасюк Я.І. &emsp;&emsp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> та групи  КА-76  &emsp;&emsp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> Іванов С. &emsp;&emsp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> Перевірила: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;</p>\n",
    "### <p style=\"text-align: right; margin: 0;\"> Дідковська М.В. &emsp;&emsp;</p>\n",
    "<br>\n",
    "\n",
    "### <p style=\"text-align: center; margin: 0;\"> Київ – 2020 </p>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Завдання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кожен учасник або учасниця команди спершу обирає дескриптор (один із розглянутих у лекціях або ж знайдений окремо) та предмет на прикладі якого відбуватиметься дослідження. Враховуючи що вас багато, будь ласка обирайте унікальніші предмети за улюблену чашку/телефон/мишку. Маючи те і інше напоготові кожен учасник бригади має зняти не менше сотні фото предмета, варіюючи його розміщення та ракурс в кадрі, освітлення, наявність візуальних перешкод, зашакаленість зображення, фокусну віддаль та тремтіння рук. Сотня фото обраного предмету на однаковій сцені з однаковою якістю зйомки, але з різних ракурсів на жаль не підійде, постарайтесь наполегливо варіювати сцени і умови зйомки. До цих фото варто додати невелику підбірку зображень, що не містять предмет, або ж містять предмет візуально подібний до вашого, штук 20 повинно вистачити, якщо залишиться натхнення можна й більше. Після чого ми нарешті дійшли до цікавого, а саме до дослідження:\n",
    "Вам потрібно згенерувати обраний дескриптор для обраного предмета, після чого з його допомогою розпізнати об’єкт на всій тестовій вибірці збираючи при цьому такі метрики: відносна кількість правильно суміщених ознак, похибка локалізації (відстань між реальним розміщенням предмета в кадрі та розпізнаним) та відносний час обробки фото в залежності від розміру зображення. Метрики мають зберегтись у файлику для подальших досліджень. \n",
    "Наступним кроком ви обмінюєтесь об’єктом з колегою, і уже маючи готову збиралку метрик, обчислюєте їх для предмета вашого сусіда, таким чином у вас збирається 9 наборів даних, по три на дескриптор. \n",
    "Самою ж ідеєю лаби є дослідити розбіжності у роботі ваших дескрипторів та виконати порівняльний аналіз їх поведінки, сформулювати висновки з викладками і прикладами так аби було зрозуміло вам та, сподіваюсь, усім вашим колегам. Таким чином кінцевим результатом буде від вас гуглдок з описом виняткових особливостей, сильних та слабких сторін дескриптора і обгрунтуванням чому вони поводяться саме так.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Хід роботи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вибірки об'єктів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 \n",
    "\n",
    "#DUCK_DS\n",
    "duck_train_pth = glob.glob(\"duck_ds/train/*.jpg\")\n",
    "duck_test_pth = glob.glob(\"duck_ds/test/*.jpg\")\n",
    "duck_ds_train = [cv2.imread(_) for _ in duck_train_pth]\n",
    "duck_ds_test = [cv2.imread(_) for _ in duck_test_pth]\n",
    "\n",
    "#CUP_DS\n",
    "cup_train_pth = glob.glob(\"cup_ds/train/*.jpg\")\n",
    "cup_test_pth = glob.glob(\"cup_ds/test/*.jpg\")\n",
    "cup_ds_train = [cv2.imread(_) for _ in cup_train_pth]\n",
    "cup_ds_test = [cv2.imread(_) for _ in cup_test_pth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дескриптори"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORB (Oriented FAST and Rotated BRIEF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import time\n",
    "    \n",
    "def ORB_descript(query_img, train_img, show_final_img=False):    \n",
    "    # Read the query image as query_img \n",
    "    # and traing image This query image \n",
    "    # is what you need to find in train image \n",
    "   \n",
    "    # Convert it to grayscale \n",
    "    query_img_bw = cv2.cvtColor(query_img,cv2.COLOR_BGR2GRAY) \n",
    "    train_img_bw = cv2.cvtColor(train_img, cv2.COLOR_BGR2GRAY) \n",
    "   \n",
    "    # Initialize the ORB detector algorithm \n",
    "    orb = cv2.ORB_create() \n",
    "   \n",
    "    t_start = time.time()\n",
    "\n",
    "    # Now detect the keypoints and compute \n",
    "    # the descriptors for the query image \n",
    "    # and train image \n",
    "    queryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw,None) \n",
    "    trainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw,None) \n",
    "  \n",
    "    if(queryDescriptors is None or trainDescriptors is None):\n",
    "        return 0, 966, 0\n",
    "    \n",
    "    # Initialize the Matcher for matching \n",
    "    # the keypoints and then match the \n",
    "    # keypoints \n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) \n",
    "    matches = matcher.match(queryDescriptors,trainDescriptors) \n",
    "\n",
    "    t_end = time.time()\n",
    "    rel_time = t_end - t_start\n",
    "    \n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    if(show_final_img):\n",
    "        # draw the matches to the final image \n",
    "        # containing both the images the drawMatches() \n",
    "        # function takes both images and keypoints \n",
    "        # and outputs the matched query image with \n",
    "        # its train image \n",
    "        final_img = cv2.drawMatches(query_img, queryKeypoints,  \n",
    "        train_img, trainKeypoints, matches[:20],None) \n",
    "   \n",
    "        final_img = cv2.resize(final_img, (1000,650)) \n",
    "  \n",
    "        # Show the final image \n",
    "        cv2.imshow(\"Matches\", final_img) \n",
    "        cv2.waitKey(3000)\n",
    "    \n",
    "    # Metrics\n",
    "    # the relative number of correctly matched features\n",
    "    rel_numb = len(matches)*2 / (len(queryDescriptors) + len(trainDescriptors))    \n",
    "    \n",
    "    # localization inaccuracy\n",
    "    distances = [_.distance for _ in matches]\n",
    "    avr_dist = sum(distances)/len(distances)\n",
    "    \n",
    "    return rel_numb, avr_dist, rel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRIEF_descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "import time\n",
    "\n",
    "\n",
    "def BRIEF_descript(query_img, train_img, show_final_img=False):    \n",
    "    # Read the query image as query_img \n",
    "    # and train image This query image \n",
    "    # is what you need to find in train image \n",
    "   \n",
    "    # Convert it to grayscale \n",
    "    query_img_bw = cv2.cvtColor(query_img,cv2.COLOR_BGR2GRAY) \n",
    "    train_img_bw = cv2.cvtColor(train_img, cv2.COLOR_BGR2GRAY) \n",
    "    #--------------------------------------------------------------------\n",
    "    # Initiate FAST detector\n",
    "    star = cv2.xfeatures2d.StarDetector_create()\n",
    "    # Initiate BRIEF extractor\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "   \n",
    "    t_start = time.time()\n",
    "\n",
    "    # Now detect the keypoints and compute \n",
    "    # the descriptors for the query image \n",
    "    # and train image \n",
    "    # find the keypoints with STAR\n",
    "    queryKeypoints = star.detect(query_img_bw,None)\n",
    "    trainKeypoints = star.detect(train_img_bw,None)\n",
    "    # compute the descriptors with BRIEF\n",
    "    queryKeypoints, queryDescriptors = brief.compute(query_img_bw, queryKeypoints)\n",
    "    trainKeypoints, trainDescriptors = brief.compute(train_img_bw,  trainKeypoints)\n",
    "    \n",
    "    #--------------------------------------------------------------------\n",
    "    if(queryDescriptors is None or trainDescriptors is None):\n",
    "        return 0, 966, 0\n",
    "\n",
    "    # Initialize the Matcher for matching \n",
    "    # the keypoints and then match the \n",
    "    # keypoints \n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) \n",
    "    matches = matcher.match(queryDescriptors,trainDescriptors) \n",
    "\n",
    "    t_end = time.time()\n",
    "    rel_time = t_end - t_start\n",
    "    \n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    \n",
    "    if(show_final_img):\n",
    "        # draw the matches to the final image \n",
    "        # containing both the images the drawMatches() \n",
    "        # function takes both images and keypoints \n",
    "        # and outputs the matched query image with \n",
    "        # its train image \n",
    "        final_img = cv2.drawMatches(query_img, queryKeypoints,  \n",
    "        train_img, trainKeypoints, matches[:20],None) \n",
    "   \n",
    "        final_img = cv2.resize(final_img, (1000,650)) \n",
    "  \n",
    "        # Show the final image \n",
    "        cv2.imshow(\"Matches\", final_img) \n",
    "        cv2.waitKey(3000)\n",
    "    \n",
    "    # Metrics\n",
    "    # the relative number of correctly matched features\n",
    "    rel_numb = len(matches)*2 / (len(queryDescriptors) + len(trainDescriptors))    \n",
    "    \n",
    "    # localization inaccuracy\n",
    "    distances = [_.distance for _ in matches]\n",
    "    avr_dist = sum(distances)/len(distances)\n",
    "    \n",
    "    return rel_numb, avr_dist, rel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from pathlib import Path\n",
    "\n",
    "def get_metrics(descriptor, ds_train, ds_test, ds_name, descr_name, plotting_graphs=True):\n",
    "    \"\"\"It creates files with \"the relative number of correctly matched features\" and \"localization inaccuracy\" metrics\"\"\"\n",
    "    \n",
    "    rel_numb_of_correct_mathced_feat=[]\n",
    "    distances=[]\n",
    "    relative_time=[]\n",
    "    \n",
    "    for train_img in ds_train:\n",
    "        distances_to_train_img =[]\n",
    "        rel_numb_for_train_img =[]\n",
    "        time_for_train_img =[]\n",
    "        \n",
    "        for test_img in ds_test:\n",
    "            rel_numb, avr_dist, time = descriptor(train_img, test_img)\n",
    "        \n",
    "            rel_numb_for_train_img.append(rel_numb)\n",
    "            distances_to_train_img.append(avr_dist)\n",
    "            time_for_train_img.append(time)\n",
    "        \n",
    "        rel_numb_of_correct_mathced_feat.append(rel_numb_for_train_img)    \n",
    "        distances.append(distances_to_train_img)\n",
    "        relative_time.append(time_for_train_img)\n",
    "        \n",
    "    # Create dirs for output data\n",
    "    Path(f\"{ds_name}/metrics/{descr_name}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"{ds_name}/metrics/{descr_name}/graphs\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # write metrics for every train img\n",
    "    for i in range(len(ds_train)):\n",
    "        with open(f\"{ds_name}/metrics/{descr_name}/data/train_img{i}_rel_numb.txt\", \"w\") as file1: \n",
    "            file1.write(\"\\n\".join([str(_) for _ in rel_numb_of_correct_mathced_feat[i]]))\n",
    "        with open(f\"{ds_name}/metrics/{descr_name}/data/train_img{i}_avr_dist.txt\", \"w\") as file2:  \n",
    "            file2.write(\"\\n\".join([str(_) for _ in distances[i]]))\n",
    "        with open(f\"{ds_name}/metrics/{descr_name}/data/train_img{i}_relative_time.txt\", \"w\") as file3:  \n",
    "            file2.write(\"\\n\".join([str(_) for _ in relative_time[i]]))\n",
    "    \n",
    "    \n",
    "    if(plotting_graphs):\n",
    "        arr=[_ for _ in range(len(ds_test))]\n",
    "        \n",
    "        for i in range(len(ds_train)):\n",
    "            output_file(f\"{ds_name}/metrics/{descr_name}/graphs/train_img{i}_rel_numb_graph.html\")\n",
    "            p = figure(title=f\"the relative number of correctly matched features from train_img_{i}\" , x_axis_label='test_img_id', y_axis_label='rel numb')\n",
    "            p.line(arr, rel_numb_of_correct_mathced_feat[i], legend_label=\"Temp.\", line_width=2)\n",
    "            show(p)\n",
    "        \n",
    "        for i in range(len(ds_train)):\n",
    "            output_file(f\"{ds_name}/metrics/{descr_name}/graphs/train_img{i}_distance_graph.html\")\n",
    "            p = figure(title=f\"Distance to train_img_{i}\" , x_axis_label='test_img_id', y_axis_label='distance')\n",
    "            p.line(arr, distances[i], legend_label=\"Temp.\", line_width=2)\n",
    "            show(p)\n",
    "        \n",
    "        for i in range(len(ds_train)):\n",
    "            output_file(f\"{ds_name}/metrics/{descr_name}/graphs/train_img{i}_relative_time_graph.html\")\n",
    "            p = figure(title=f\"Relative time for train_img_{i}\" , x_axis_label='test_img_id', y_axis_label='relative_time')\n",
    "            p.line(arr, relative_time[i], legend_label=\"Temp.\", line_width=2)\n",
    "            show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отримання метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d43f28a48416>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mORB_descript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduck_ds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduck_ds_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"duck_ds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a27e58c30f7c>\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(descriptor, ds_train, ds_test, ds_name, plotting_graphs)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mfile2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{ds_name}/metrics/data/train_img{i}_relative_time.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mfile2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrelative_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "get_metrics(ORB_descript, duck_ds_train, duck_ds_test, \"duck_ds\", \"ORB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(ORB_descript, cup_ds_train, cup_ds_test, \"duck_ds\", \"ORB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(BRIEF_descript, cup_ds_train, cup_ds_test, \"cup_ds\", \"BRIEF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(BRIEF_descript, duck_ds_train, duck_ds_test, \"duck_ds\", \"BRIEF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результати"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримані метрики можно побачити у файлах \\[Назва датасету\\]\\_\\[Метрика\\]  \n",
    "Граффіки на основі отриманих даних \\[Назва датасету\\]\\_\\[Метрика\\]\\_\\[Номер тренованого зображення в масиві\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб вивести дескпиптор конкретного зображення, введіть номер тренованого й тестового зображення у масиві"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_numb, avr_dist, rel_time  = ORB_descript(duck_ds_train[1], duck_ds_test[40], show_final_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналіз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
